# **AI Code Review System**

##  **Общая информация**

Этот проект представляет собой **систему автоматической проверки кода** с использованием двух микросервисов: **AI Reviewer** и **CodeSage**. Система интегрируется с **GitLab** для автоматической проверки Merge Requests, анализа диффов, выявления ошибок в коде и генерации отзывов с помощью AI.

Проект использует **Go** для реализации серверной логики, а также интеграцию с **Ollama** для генерации отзывов на основе LLaMA.

---

##  **Структура проекта**

Проект состоит из двух микросервисов:

### 1. **AI Reviewer Service**
   - **Слушает** Webhook-события GitLab (Merge Request Hook).
   - **Получает** дифф кода через GitLab API.
   - **Отправляет** дифф в **CodeSage** для статического анализа.
   - **Генерирует** отзыв с использованием LLaMA через Ollama API.
   - **Оставляет** комментарий в Merge Request.



##  **CodeSage vs LLaMA: Почему CodeSage лучше для анализа кода**

**CodeSage** — это специализированный сервис для **статического анализа кода**, который выполняет два ключевых типа проверок:

-  **AST-анализ** для синтаксических ошибок.
-  **Lint-анализ** с помощью **golangci-lint** для проверки ошибок форматирования и стиля.

###  **Почему это лучше, чем просто использовать LLaMA**:

1. **Точный анализ**:
   - **LLaMA** генерирует текст, но не понимает структуры кода. Она не может обнаружить такие ошибки, как **незакрытые скобки** или **неиспользуемые переменные**, которые легко выявляются с помощью **CodeSage**.
   
2. **Контекст изменений**:
   - **CodeSage** анализирует только **диффы** (изменения в коде), что позволяет делать точные рекомендации. В отличие от этого, **LLaMA** не учитывает контекст изменений и может генерировать общие или неактуальные комментарии.

3. **Точные рекомендации**:
   - **CodeSage** использует **golangci-lint** для поиска **конкретных линтинговых ошибок** и нарушений стиля, что позволяет давать более точные и полезные рекомендации по улучшению кода.

---

###  **Почему сочетание CodeSage + LLaMA — это идеальное решение**:

- **CodeSage** выполняет глубокий **анализ кода** и выявляет ошибки на синтаксическом и структурном уровне.
- **LLaMA** генерирует **умные** комментарии и рекомендации, основываясь на результатах анализа, что делает отзывы точными и полезными для разработчиков.

---

 **В результате** использование **CodeSage** и **LLaMA** вместе обеспечивает **надежный и точный** процесс проверки кода, улучшая качество и скорость разработки.

---




##  **Поток данных (Pipeline)**

1. **GitLab MR Event**
2. **AI Reviewer** (Go)
   - Получает дифф с GitLab API
   - Отправляет дифф в **CodeSage** для анализа
3. **CodeSage** (Go)
   - Выполняет **AST** и **Lint** анализ
4. **AI Reviewer**
   - Формирует запрос для **LLaMA**
   - Отправляет запрос в **Ollama**
5. **LLaMA** (через Ollama API)
   - Получает запрос и генерирует текст отзыва
6. **AI Reviewer**
   - Получает отзыв от LLaMA
   - Публикует комментарий в Merge Request в GitLab

---

##  **Управление окружением**

### Настройки для **AI Reviewer**

Создайте файл `.env` с следующими переменными:

## env
GITLAB_BASE_URL=https://gitlab.com
GITLAB_TOKEN=YOUR_TOKEN
GITLAB_WEBHOOK_SECRET=YOUR_SECRET
PORT=8080
CODESAGE_URL=http://codesage:8081/analyze
LLAMA_URL=http://host.docker.internal:11434/api/generate


# **AI Code Review System**
##  **Как запустить проект**


   1. Установите **Ollama**
   Скачайте **Ollama** с [официального сайта](https://ollama.com/download).
После установки выполните команду, чтобы проверить установку:
## Terminal
**ollama list**
ollama pull llama3.1


   2. Запустите сервер Ollama
Запустите локальный сервер Ollama для работы с LLaMA:
**ollama serve**


   3. Выполните go mod tidy
После того как вы клонировали репозиторий, выполните команду для установки всех зависимостей:
**go mod tidy**


   5. Поднимите Docker-сервисы
В корне проекта выполните команду для поднятия двух сервисов с помощью Docker Compose:
**docker compose up --build -d**
Эта команда создаст и запустит два контейнера:
AI Reviewer: сервис для анализа и генерации отзывов с использованием LLaMA.
CodeSage: сервис для статического анализа Go-кода (AST и линтинг).


   6. Настроить ngrok для локального тестирования
Чтобы тестировать проект локально, вам нужно проксировать локальные порты через ngrok:
Установите ngrok с официального сайта.
Запустите ngrok на порту, который используется для Webhook в GitLab:

**ngrok http 8080
Это создаст публичный URL, который будет перенаправлять трафик на ваш локальный сервер.**


## TeamPlay:

**Нурдаулет Хаймульдин - Go BackEnd Developer**

**Бакеев Аскар - MLOps Engineer https://askarbakeyev-248376.gitlab.io/**

**Жания Жакипова - Go BackEnd Developer**

**Максат Макашев - Project Manager**

## Check our GitHubs:

https://github.com/askarrrr04

https://github.com/NuKAHAHA
